MLPMIXER CONFIGURATION
----------------------------------------------------------------
num_classes:	10
img_size:	32
patch_size:	4
embed_dim:	512
num_blocks:	8
drop_rate:	0.0
activation:	gelu
mlp_ratio:	[0.5, 4.0]
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 512, 8, 8]          25,088
        PatchEmbed-2              [-1, 64, 512]               0
         LayerNorm-3              [-1, 64, 512]           1,024
            Linear-4             [-1, 512, 256]          16,640
              GELU-5             [-1, 512, 256]               0
           Dropout-6             [-1, 512, 256]               0
            Linear-7              [-1, 512, 64]          16,448
           Dropout-8              [-1, 512, 64]               0
               Mlp-9              [-1, 512, 64]               0
        LayerNorm-10              [-1, 64, 512]           1,024
           Linear-11             [-1, 64, 2048]       1,050,624
             GELU-12             [-1, 64, 2048]               0
          Dropout-13             [-1, 64, 2048]               0
           Linear-14              [-1, 64, 512]       1,049,088
          Dropout-15              [-1, 64, 512]               0
              Mlp-16              [-1, 64, 512]               0
       MixerBlock-17              [-1, 64, 512]               0
        LayerNorm-18              [-1, 64, 512]           1,024
           Linear-19             [-1, 512, 256]          16,640
             GELU-20             [-1, 512, 256]               0
          Dropout-21             [-1, 512, 256]               0
           Linear-22              [-1, 512, 64]          16,448
          Dropout-23              [-1, 512, 64]               0
              Mlp-24              [-1, 512, 64]               0
        LayerNorm-25              [-1, 64, 512]           1,024
           Linear-26             [-1, 64, 2048]       1,050,624
             GELU-27             [-1, 64, 2048]               0
          Dropout-28             [-1, 64, 2048]               0
           Linear-29              [-1, 64, 512]       1,049,088
          Dropout-30              [-1, 64, 512]               0
              Mlp-31              [-1, 64, 512]               0
       MixerBlock-32              [-1, 64, 512]               0
        LayerNorm-33              [-1, 64, 512]           1,024
           Linear-34             [-1, 512, 256]          16,640
             GELU-35             [-1, 512, 256]               0
          Dropout-36             [-1, 512, 256]               0
           Linear-37              [-1, 512, 64]          16,448
          Dropout-38              [-1, 512, 64]               0
              Mlp-39              [-1, 512, 64]               0
        LayerNorm-40              [-1, 64, 512]           1,024
           Linear-41             [-1, 64, 2048]       1,050,624
             GELU-42             [-1, 64, 2048]               0
          Dropout-43             [-1, 64, 2048]               0
           Linear-44              [-1, 64, 512]       1,049,088
          Dropout-45              [-1, 64, 512]               0
              Mlp-46              [-1, 64, 512]               0
       MixerBlock-47              [-1, 64, 512]               0
        LayerNorm-48              [-1, 64, 512]           1,024
           Linear-49             [-1, 512, 256]          16,640
             GELU-50             [-1, 512, 256]               0
          Dropout-51             [-1, 512, 256]               0
           Linear-52              [-1, 512, 64]          16,448
          Dropout-53              [-1, 512, 64]               0
              Mlp-54              [-1, 512, 64]               0
        LayerNorm-55              [-1, 64, 512]           1,024
           Linear-56             [-1, 64, 2048]       1,050,624
             GELU-57             [-1, 64, 2048]               0
          Dropout-58             [-1, 64, 2048]               0
           Linear-59              [-1, 64, 512]       1,049,088
          Dropout-60              [-1, 64, 512]               0
              Mlp-61              [-1, 64, 512]               0
       MixerBlock-62              [-1, 64, 512]               0
        LayerNorm-63              [-1, 64, 512]           1,024
           Linear-64             [-1, 512, 256]          16,640
             GELU-65             [-1, 512, 256]               0
          Dropout-66             [-1, 512, 256]               0
           Linear-67              [-1, 512, 64]          16,448
          Dropout-68              [-1, 512, 64]               0
              Mlp-69              [-1, 512, 64]               0
        LayerNorm-70              [-1, 64, 512]           1,024
           Linear-71             [-1, 64, 2048]       1,050,624
             GELU-72             [-1, 64, 2048]               0
          Dropout-73             [-1, 64, 2048]               0
           Linear-74              [-1, 64, 512]       1,049,088
          Dropout-75              [-1, 64, 512]               0
              Mlp-76              [-1, 64, 512]               0
       MixerBlock-77              [-1, 64, 512]               0
        LayerNorm-78              [-1, 64, 512]           1,024
           Linear-79             [-1, 512, 256]          16,640
             GELU-80             [-1, 512, 256]               0
          Dropout-81             [-1, 512, 256]               0
           Linear-82              [-1, 512, 64]          16,448
          Dropout-83              [-1, 512, 64]               0
              Mlp-84              [-1, 512, 64]               0
        LayerNorm-85              [-1, 64, 512]           1,024
           Linear-86             [-1, 64, 2048]       1,050,624
             GELU-87             [-1, 64, 2048]               0
          Dropout-88             [-1, 64, 2048]               0
           Linear-89              [-1, 64, 512]       1,049,088
          Dropout-90              [-1, 64, 512]               0
              Mlp-91              [-1, 64, 512]               0
       MixerBlock-92              [-1, 64, 512]               0
        LayerNorm-93              [-1, 64, 512]           1,024
           Linear-94             [-1, 512, 256]          16,640
             GELU-95             [-1, 512, 256]               0
          Dropout-96             [-1, 512, 256]               0
           Linear-97              [-1, 512, 64]          16,448
          Dropout-98              [-1, 512, 64]               0
              Mlp-99              [-1, 512, 64]               0
       LayerNorm-100              [-1, 64, 512]           1,024
          Linear-101             [-1, 64, 2048]       1,050,624
            GELU-102             [-1, 64, 2048]               0
         Dropout-103             [-1, 64, 2048]               0
          Linear-104              [-1, 64, 512]       1,049,088
         Dropout-105              [-1, 64, 512]               0
             Mlp-106              [-1, 64, 512]               0
      MixerBlock-107              [-1, 64, 512]               0
       LayerNorm-108              [-1, 64, 512]           1,024
          Linear-109             [-1, 512, 256]          16,640
            GELU-110             [-1, 512, 256]               0
         Dropout-111             [-1, 512, 256]               0
          Linear-112              [-1, 512, 64]          16,448
         Dropout-113              [-1, 512, 64]               0
             Mlp-114              [-1, 512, 64]               0
       LayerNorm-115              [-1, 64, 512]           1,024
          Linear-116             [-1, 64, 2048]       1,050,624
            GELU-117             [-1, 64, 2048]               0
         Dropout-118             [-1, 64, 2048]               0
          Linear-119              [-1, 64, 512]       1,049,088
         Dropout-120              [-1, 64, 512]               0
             Mlp-121              [-1, 64, 512]               0
      MixerBlock-122              [-1, 64, 512]               0
       LayerNorm-123              [-1, 64, 512]           1,024
          Linear-124                   [-1, 10]           5,130
================================================================
Total params: 17,110,026
Trainable params: 17,110,026
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 66.75
Params size (MB): 65.27
Estimated Total Size (MB): 132.03
----------------------------------------------------------------
