### mlpmixer_baseline

MLPMIXER CONFIGURATION
----------------------------------------------------------------
num_classes:	10
img_size:	32
patch_size:	4
embed_dim:	256
num_blocks:	4
drop_rate:	0.0
activation:	gelu
mlp_ratio:	[0.5, 4.0]
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 256, 8, 8]          12,544
        PatchEmbed-2              [-1, 64, 256]               0
         LayerNorm-3              [-1, 64, 256]             512
            Linear-4             [-1, 256, 128]           8,320
              GELU-5             [-1, 256, 128]               0
           Dropout-6             [-1, 256, 128]               0
            Linear-7              [-1, 256, 64]           8,256
           Dropout-8              [-1, 256, 64]               0
               Mlp-9              [-1, 256, 64]               0
        LayerNorm-10              [-1, 64, 256]             512
           Linear-11             [-1, 64, 1024]         263,168
             GELU-12             [-1, 64, 1024]               0
          Dropout-13             [-1, 64, 1024]               0
           Linear-14              [-1, 64, 256]         262,400
          Dropout-15              [-1, 64, 256]               0
              Mlp-16              [-1, 64, 256]               0
       MixerBlock-17              [-1, 64, 256]               0
        LayerNorm-18              [-1, 64, 256]             512
           Linear-19             [-1, 256, 128]           8,320
             GELU-20             [-1, 256, 128]               0
          Dropout-21             [-1, 256, 128]               0
           Linear-22              [-1, 256, 64]           8,256
          Dropout-23              [-1, 256, 64]               0
              Mlp-24              [-1, 256, 64]               0
        LayerNorm-25              [-1, 64, 256]             512
           Linear-26             [-1, 64, 1024]         263,168
             GELU-27             [-1, 64, 1024]               0
          Dropout-28             [-1, 64, 1024]               0
           Linear-29              [-1, 64, 256]         262,400
          Dropout-30              [-1, 64, 256]               0
              Mlp-31              [-1, 64, 256]               0
       MixerBlock-32              [-1, 64, 256]               0
        LayerNorm-33              [-1, 64, 256]             512
           Linear-34             [-1, 256, 128]           8,320
             GELU-35             [-1, 256, 128]               0
          Dropout-36             [-1, 256, 128]               0
           Linear-37              [-1, 256, 64]           8,256
          Dropout-38              [-1, 256, 64]               0
              Mlp-39              [-1, 256, 64]               0
        LayerNorm-40              [-1, 64, 256]             512
           Linear-41             [-1, 64, 1024]         263,168
             GELU-42             [-1, 64, 1024]               0
          Dropout-43             [-1, 64, 1024]               0
           Linear-44              [-1, 64, 256]         262,400
          Dropout-45              [-1, 64, 256]               0
              Mlp-46              [-1, 64, 256]               0
       MixerBlock-47              [-1, 64, 256]               0
        LayerNorm-48              [-1, 64, 256]             512
           Linear-49             [-1, 256, 128]           8,320
             GELU-50             [-1, 256, 128]               0
          Dropout-51             [-1, 256, 128]               0
           Linear-52              [-1, 256, 64]           8,256
          Dropout-53              [-1, 256, 64]               0
              Mlp-54              [-1, 256, 64]               0
        LayerNorm-55              [-1, 64, 256]             512
           Linear-56             [-1, 64, 1024]         263,168
             GELU-57             [-1, 64, 1024]               0
          Dropout-58             [-1, 64, 1024]               0
           Linear-59              [-1, 64, 256]         262,400
          Dropout-60              [-1, 64, 256]               0
              Mlp-61              [-1, 64, 256]               0
       MixerBlock-62              [-1, 64, 256]               0
        LayerNorm-63              [-1, 64, 256]             512
           Linear-64                   [-1, 10]           2,570
================================================================
Total params: 2,188,298
Trainable params: 2,188,298
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 13.88
Params size (MB): 8.35
Estimated Total Size (MB): 22.23
----------------------------------------------------------------


### mlpmixer_increased_token

MLPMIXER CONFIGURATION
----------------------------------------------------------------
num_classes:	10
img_size:	32
patch_size:	4
embed_dim:	256
num_blocks:	4
drop_rate:	0.0
activation:	gelu
mlp_ratio:	[1.0, 4.0]
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 256, 8, 8]          12,544
        PatchEmbed-2              [-1, 64, 256]               0
         LayerNorm-3              [-1, 64, 256]             512
            Linear-4             [-1, 256, 256]          16,640
              GELU-5             [-1, 256, 256]               0
           Dropout-6             [-1, 256, 256]               0
            Linear-7              [-1, 256, 64]          16,448
           Dropout-8              [-1, 256, 64]               0
               Mlp-9              [-1, 256, 64]               0
        LayerNorm-10              [-1, 64, 256]             512
           Linear-11             [-1, 64, 1024]         263,168
             GELU-12             [-1, 64, 1024]               0
          Dropout-13             [-1, 64, 1024]               0
           Linear-14              [-1, 64, 256]         262,400
          Dropout-15              [-1, 64, 256]               0
              Mlp-16              [-1, 64, 256]               0
       MixerBlock-17              [-1, 64, 256]               0
        LayerNorm-18              [-1, 64, 256]             512
           Linear-19             [-1, 256, 256]          16,640
             GELU-20             [-1, 256, 256]               0
          Dropout-21             [-1, 256, 256]               0
           Linear-22              [-1, 256, 64]          16,448
          Dropout-23              [-1, 256, 64]               0
              Mlp-24              [-1, 256, 64]               0
        LayerNorm-25              [-1, 64, 256]             512
           Linear-26             [-1, 64, 1024]         263,168
             GELU-27             [-1, 64, 1024]               0
          Dropout-28             [-1, 64, 1024]               0
           Linear-29              [-1, 64, 256]         262,400
          Dropout-30              [-1, 64, 256]               0
              Mlp-31              [-1, 64, 256]               0
       MixerBlock-32              [-1, 64, 256]               0
        LayerNorm-33              [-1, 64, 256]             512
           Linear-34             [-1, 256, 256]          16,640
             GELU-35             [-1, 256, 256]               0
          Dropout-36             [-1, 256, 256]               0
           Linear-37              [-1, 256, 64]          16,448
          Dropout-38              [-1, 256, 64]               0
              Mlp-39              [-1, 256, 64]               0
        LayerNorm-40              [-1, 64, 256]             512
           Linear-41             [-1, 64, 1024]         263,168
             GELU-42             [-1, 64, 1024]               0
          Dropout-43             [-1, 64, 1024]               0
           Linear-44              [-1, 64, 256]         262,400
          Dropout-45              [-1, 64, 256]               0
              Mlp-46              [-1, 64, 256]               0
       MixerBlock-47              [-1, 64, 256]               0
        LayerNorm-48              [-1, 64, 256]             512
           Linear-49             [-1, 256, 256]          16,640
             GELU-50             [-1, 256, 256]               0
          Dropout-51             [-1, 256, 256]               0
           Linear-52              [-1, 256, 64]          16,448
          Dropout-53              [-1, 256, 64]               0
              Mlp-54              [-1, 256, 64]               0
        LayerNorm-55              [-1, 64, 256]             512
           Linear-56             [-1, 64, 1024]         263,168
             GELU-57             [-1, 64, 1024]               0
          Dropout-58             [-1, 64, 1024]               0
           Linear-59              [-1, 64, 256]         262,400
          Dropout-60              [-1, 64, 256]               0
              Mlp-61              [-1, 64, 256]               0
       MixerBlock-62              [-1, 64, 256]               0
        LayerNorm-63              [-1, 64, 256]             512
           Linear-64                   [-1, 10]           2,570
================================================================
Total params: 2,254,346
Trainable params: 2,254,346
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 16.88
Params size (MB): 8.60
Estimated Total Size (MB): 25.49
----------------------------------------------------------------


### mlpmixer_increased_channel

MLPMIXER CONFIGURATION
----------------------------------------------------------------
num_classes:	10
img_size:	32
patch_size:	4
embed_dim:	256
num_blocks:	4
drop_rate:	0.0
activation:	gelu
mlp_ratio:	[0.5, 8.0]
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 256, 8, 8]          12,544
        PatchEmbed-2              [-1, 64, 256]               0
         LayerNorm-3              [-1, 64, 256]             512
            Linear-4             [-1, 256, 128]           8,320
              GELU-5             [-1, 256, 128]               0
           Dropout-6             [-1, 256, 128]               0
            Linear-7              [-1, 256, 64]           8,256
           Dropout-8              [-1, 256, 64]               0
               Mlp-9              [-1, 256, 64]               0
        LayerNorm-10              [-1, 64, 256]             512
           Linear-11             [-1, 64, 2048]         526,336
             GELU-12             [-1, 64, 2048]               0
          Dropout-13             [-1, 64, 2048]               0
           Linear-14              [-1, 64, 256]         524,544
          Dropout-15              [-1, 64, 256]               0
              Mlp-16              [-1, 64, 256]               0
       MixerBlock-17              [-1, 64, 256]               0
        LayerNorm-18              [-1, 64, 256]             512
           Linear-19             [-1, 256, 128]           8,320
             GELU-20             [-1, 256, 128]               0
          Dropout-21             [-1, 256, 128]               0
           Linear-22              [-1, 256, 64]           8,256
          Dropout-23              [-1, 256, 64]               0
              Mlp-24              [-1, 256, 64]               0
        LayerNorm-25              [-1, 64, 256]             512
           Linear-26             [-1, 64, 2048]         526,336
             GELU-27             [-1, 64, 2048]               0
          Dropout-28             [-1, 64, 2048]               0
           Linear-29              [-1, 64, 256]         524,544
          Dropout-30              [-1, 64, 256]               0
              Mlp-31              [-1, 64, 256]               0
       MixerBlock-32              [-1, 64, 256]               0
        LayerNorm-33              [-1, 64, 256]             512
           Linear-34             [-1, 256, 128]           8,320
             GELU-35             [-1, 256, 128]               0
          Dropout-36             [-1, 256, 128]               0
           Linear-37              [-1, 256, 64]           8,256
          Dropout-38              [-1, 256, 64]               0
              Mlp-39              [-1, 256, 64]               0
        LayerNorm-40              [-1, 64, 256]             512
           Linear-41             [-1, 64, 2048]         526,336
             GELU-42             [-1, 64, 2048]               0
          Dropout-43             [-1, 64, 2048]               0
           Linear-44              [-1, 64, 256]         524,544
          Dropout-45              [-1, 64, 256]               0
              Mlp-46              [-1, 64, 256]               0
       MixerBlock-47              [-1, 64, 256]               0
        LayerNorm-48              [-1, 64, 256]             512
           Linear-49             [-1, 256, 128]           8,320
             GELU-50             [-1, 256, 128]               0
          Dropout-51             [-1, 256, 128]               0
           Linear-52              [-1, 256, 64]           8,256
          Dropout-53              [-1, 256, 64]               0
              Mlp-54              [-1, 256, 64]               0
        LayerNorm-55              [-1, 64, 256]             512
           Linear-56             [-1, 64, 2048]         526,336
             GELU-57             [-1, 64, 2048]               0
          Dropout-58             [-1, 64, 2048]               0
           Linear-59              [-1, 64, 256]         524,544
          Dropout-60              [-1, 64, 256]               0
              Mlp-61              [-1, 64, 256]               0
       MixerBlock-62              [-1, 64, 256]               0
        LayerNorm-63              [-1, 64, 256]             512
           Linear-64                   [-1, 10]           2,570
================================================================
Total params: 4,289,546
Trainable params: 4,289,546
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 19.88
Params size (MB): 16.36
Estimated Total Size (MB): 36.25
----------------------------------------------------------------


### mlpmixer_balanced_ratio

MLPMIXER CONFIGURATION
----------------------------------------------------------------
num_classes:	10
img_size:	32
patch_size:	4
embed_dim:	256
num_blocks:	4
drop_rate:	0.0
activation:	gelu
mlp_ratio:	[1, 1]
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 256, 8, 8]          12,544
        PatchEmbed-2              [-1, 64, 256]               0
         LayerNorm-3              [-1, 64, 256]             512
            Linear-4             [-1, 256, 256]          16,640
              GELU-5             [-1, 256, 256]               0
           Dropout-6             [-1, 256, 256]               0
            Linear-7              [-1, 256, 64]          16,448
           Dropout-8              [-1, 256, 64]               0
               Mlp-9              [-1, 256, 64]               0
        LayerNorm-10              [-1, 64, 256]             512
           Linear-11              [-1, 64, 256]          65,792
             GELU-12              [-1, 64, 256]               0
          Dropout-13              [-1, 64, 256]               0
           Linear-14              [-1, 64, 256]          65,792
          Dropout-15              [-1, 64, 256]               0
              Mlp-16              [-1, 64, 256]               0
       MixerBlock-17              [-1, 64, 256]               0
        LayerNorm-18              [-1, 64, 256]             512
           Linear-19             [-1, 256, 256]          16,640
             GELU-20             [-1, 256, 256]               0
          Dropout-21             [-1, 256, 256]               0
           Linear-22              [-1, 256, 64]          16,448
          Dropout-23              [-1, 256, 64]               0
              Mlp-24              [-1, 256, 64]               0
        LayerNorm-25              [-1, 64, 256]             512
           Linear-26              [-1, 64, 256]          65,792
             GELU-27              [-1, 64, 256]               0
          Dropout-28              [-1, 64, 256]               0
           Linear-29              [-1, 64, 256]          65,792
          Dropout-30              [-1, 64, 256]               0
              Mlp-31              [-1, 64, 256]               0
       MixerBlock-32              [-1, 64, 256]               0
        LayerNorm-33              [-1, 64, 256]             512
           Linear-34             [-1, 256, 256]          16,640
             GELU-35             [-1, 256, 256]               0
          Dropout-36             [-1, 256, 256]               0
           Linear-37              [-1, 256, 64]          16,448
          Dropout-38              [-1, 256, 64]               0
              Mlp-39              [-1, 256, 64]               0
        LayerNorm-40              [-1, 64, 256]             512
           Linear-41              [-1, 64, 256]          65,792
             GELU-42              [-1, 64, 256]               0
          Dropout-43              [-1, 64, 256]               0
           Linear-44              [-1, 64, 256]          65,792
          Dropout-45              [-1, 64, 256]               0
              Mlp-46              [-1, 64, 256]               0
       MixerBlock-47              [-1, 64, 256]               0
        LayerNorm-48              [-1, 64, 256]             512
           Linear-49             [-1, 256, 256]          16,640
             GELU-50             [-1, 256, 256]               0
          Dropout-51             [-1, 256, 256]               0
           Linear-52              [-1, 256, 64]          16,448
          Dropout-53              [-1, 256, 64]               0
              Mlp-54              [-1, 256, 64]               0
        LayerNorm-55              [-1, 64, 256]             512
           Linear-56              [-1, 64, 256]          65,792
             GELU-57              [-1, 64, 256]               0
          Dropout-58              [-1, 64, 256]               0
           Linear-59              [-1, 64, 256]          65,792
          Dropout-60              [-1, 64, 256]               0
              Mlp-61              [-1, 64, 256]               0
       MixerBlock-62              [-1, 64, 256]               0
        LayerNorm-63              [-1, 64, 256]             512
           Linear-64                   [-1, 10]           2,570
================================================================
Total params: 678,410
Trainable params: 678,410
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 12.38
Params size (MB): 2.59
Estimated Total Size (MB): 14.97
----------------------------------------------------------------


