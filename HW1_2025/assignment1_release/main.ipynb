{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fwilhelmy/IFT6135-2025/blob/main/HW1_2025/assignment1_release/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTv0D26B9W2h"
      },
      "source": [
        "# Assignment 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "qFHMMDtSwuW4",
        "outputId": "71abd7ef-71f0-4ec4-ace4-031a7511420c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        },
        {
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b7dceb212c17>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "#@title Mount your Google Drive\n",
        "# If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n",
        "# you can delete this cell which is specific to Google Colab. You may also\n",
        "# change the paths for data/logs in Arguments below.\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oODLwt1QzgGa"
      },
      "outputs": [],
      "source": [
        "#@title Link your assignment folder & install requirements\n",
        "#@markdown Enter the path to the assignment folder in your Google Drive\n",
        "# If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n",
        "# you can delete this cell which is specific to Google Colab. You may also\n",
        "# change the paths for data/logs in Arguments below.\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import warnings\n",
        "\n",
        "folder = \"/content/gdrive/MyDrive/IFT6135/HW1_2025/assignment1_release\" #@param {type:\"string\"}\n",
        "!ln -Ts \"$folder\" /content/assignment 2> /dev/null\n",
        "\n",
        "# Add the assignment folder to Python path\n",
        "if '/content/assignment' not in sys.path:\n",
        "  sys.path.insert(0, '/content/assignment')\n",
        "\n",
        "# Check if CUDA is available\n",
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "  warnings.warn('CUDA is not available.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dt3NTvpsy4Oc"
      },
      "source": [
        "### Running on GPU\n",
        "For this assignment, it will be necessary to run your experiments on GPU. To make sure the notebook is running on GPU, you can change the notebook settings with\n",
        "* (EN) `Edit > Notebook Settings`\n",
        "* (FR) `Modifier > Paramètres du notebook`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "RLVSmv9HoMH5",
        "outputId": "83bf8f8a-6a7a-4304-ed55-4cc6c6bc9784"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import warnings\n",
        "\n",
        "from main import Arguments, run_experiment\n",
        "from utils import save_model, load_model, generate_plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZy1J-0OroLg"
      },
      "source": [
        "# Local Test\n",
        "Before run the experiment, here are some local test cases you can run for sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wLEVxwLlroLh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test_linear_attributes (testy.TestLinear.test_linear_attributes) ... ok\n",
            "test_linear_forward (testy.TestLinear.test_linear_forward) ... ok\n",
            "test_activation (testy.TestMLP.test_activation) ... ok\n",
            "test_forward (testy.TestMLP.test_forward) ... ok\n",
            "test_mlp (testy.TestMLP.test_mlp) ... ok\n",
            "test_mixer_block (testy.TestMLPMixer.test_mixer_block) ... ok\n",
            "test_mlpmixer (testy.TestMLPMixer.test_mlpmixer) ... ok\n",
            "test_patch_emb (testy.TestMLPMixer.test_patch_emb) ... ok\n",
            "test_basic_block (testy.TestResNet.test_basic_block) ... ok\n",
            "test_basic_block2 (testy.TestResNet.test_basic_block2) ... ok\n",
            "test_resnet (testy.TestResNet.test_resnet) ... ok\n",
            "test_ce_loss (testy.TestUtils.test_ce_loss) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 12 tests in 1.083s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=12 errors=0 failures=0>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import unittest\n",
        "import testy\n",
        "suite = unittest.TestLoader().loadTestsFromModule(testy)\n",
        "unittest.TextTestRunner(verbosity=2).run(suite)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PtvL_yKp3PW"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 1\n",
        "\n",
        "For the MLP architecture, investigate the effect of the choice of non-linearity while keeping the other hyperparameters the same as the default. You are expected to provide four figures corresponding to training loss, validation loss, training accuracy, and validation accuracy, where the x-axis is the number of epochs. For each figure, use the legend to denote the non-linearity being used. Conclude which non-linearity is the best and give your explanation. Optionally, we provide the plotting utility function in utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyJPWO1ppcTx"
      },
      "outputs": [],
      "source": [
        "experiment1 = {\n",
        "    \"mlp_relu\": Arguments(logdir=\"results/experiment1/mlp_relu\", model='mlp', model_config='model_configs/mlp/mlp_relu.json', epochs=10),\n",
        "    \"mlp_sigmoid\": Arguments(logdir=\"results/experiment1/mlp_sigmoid\", model='mlp', model_config='model_configs/mlp/mlp_sigmoid.json', epochs=10),\n",
        "    \"mlp_tanh\": Arguments(logdir=\"results/experiment1/mlp_tanh\", model='mlp', model_config='model_configs/mlp/mlp_tanh.json', epochs=10),\n",
        "}\n",
        "\n",
        "run_experiment(experiment1)\n",
        "\n",
        "legend_names = ['ReLU', 'Sigmoid', 'Tanh']\n",
        "logdirs = [experiment1[key].logdir for key in experiment1]\n",
        "\n",
        "generate_plots(logdirs, legend_names, \"results/experiment1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!zip -r experiment1.zip results/experiment1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 2 \n",
        "\n",
        "For the ResNet18 architecture, investigate the effect of learning rate with the Adam optimizer. Perform experiments with learning rates of 0.1, 0.01, 0.001, 0.0001, 0.00001. Provide the figures and explain your findings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "experiment2 = {\n",
        "    \"resnet18_lr1e-1\": Arguments(logdir=\"results/experiment2/resnet18_lr1e-1\", model='resnet18', model_config='assignment/model_configs/resnet18.json', optimizer='adam', epochs=10, lr=1e-1),\n",
        "    \"resnet18_lr1e-2\": Arguments(logdir=\"results/experiment2/resnet18_lr1e-2\", model='resnet18', model_config='assignment/model_configs/resnet18.json', optimizer='adam', epochs=10, lr=1e-2),\n",
        "    \"resnet18_lr1e-3\": Arguments(logdir=\"results/experiment2/resnet18_lr1e-3\", model='resnet18', model_config='assignment/model_configs/resnet18.json', optimizer='adam', epochs=10, lr=1e-3),\n",
        "    \"resnet18_lr1e-4\": Arguments(logdir=\"results/experiment2/resnet18_lr1e-4\", model='resnet18', model_config='assignment/model_configs/resnet18.json', optimizer='adam', epochs=10, lr=1e-4),\n",
        "    \"resnet18_lr1e-5\": Arguments(logdir=\"results/experiment2/resnet18_lr1e-5\", model='resnet18', model_config='assignment/model_configs/resnet18.json', optimizer='adam', epochs=10, lr=1e-5),\n",
        "}\n",
        "\n",
        "run_experiment(experiment2)\n",
        "\n",
        "legend_names = [x for x in experiment2]\n",
        "logdirs = [experiment2[x].logdir for x in experiment2]\n",
        "\n",
        "generate_plots(logdirs, legend_names, \"results/experiment2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!zip -r experiment2.zip results/experiment2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 3\n",
        "\n",
        "For MLPMixer, investigate the effect of patch size. No recommended values are given, and you are expected to run at least 3 experiments. Remember there are only a few valid values for patch size for the given image size. Please provide figures and explain your findings. Also explain in text the effect on the number of model parameters and running time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "experiment3 = {\n",
        "    \"mlpmixer_patch2\": Arguments(logdir=\"results/experiment3/mlpmixer_patch2\", model='mlpmixer', model_config='model_configs/mlpmixer/mlpmixer_patch2.json', epochs=15),\n",
        "    \"mlpmixer_patch4\": Arguments(logdir=\"results/experiment3/mlpmixer_patch4\", model='mlpmixer', model_config='model_configs/mlpmixer/mlpmixer_patch4.json', epochs=15),\n",
        "    \"mlpmixer_patch8\": Arguments(logdir=\"results/experiment3/mlpmixer_patch8\", model='mlpmixer', model_config='model_configs/mlpmixer/mlpmixer_patch8.json', epochs=15)\n",
        "}\n",
        "\n",
        "run_experiment(experiment3)\n",
        "\n",
        "legend_names = [x for x in experiment3.keys()]\n",
        "logdirs = [experiment3[x].logdir for x in experiment3]\n",
        "\n",
        "generate_plots(logdirs, legend_names, \"results/experiment3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!zip -r experiment3.zip results/experiment3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 4\n",
        "\n",
        "Find your best ResNet18 model by experimenting with different hyper-parameter choices. Provide the hyperparameters in your report. Visualize the kernels of the first layer, which has a weight of shape (out channel, in channel, kernel size, kernel size). You can modify the main.py or add extra cell in main.ipynb for visualization. Since we have 64 output channels and 3 input channels (RGB), one can view this as sixty-four 3 × 3 small images, where each image represent the kernel corresponding to that output channel. Note that this is an open-ended question. You can perform different pre-processing for visualization, e.g., standardizing the weight values, averaging across the channels to have gray scale images etc. You can see more details and examples in this blogpost. Please describe your visualization procedure in your report.\n",
        "\n",
        "---\n",
        "\n",
        "### SGD Without Momentum\n",
        "Empirical evidence over the years shows that training deep CNNs like ResNet18 using plain SGD (i.e., without momentum) generally converges more slowly and may get stuck in poor local minima. Momentum helps accelerate SGD in the relevant direction and dampens oscillations, resulting in faster convergence and often better final performance.Given that momentum is a simple addition (and a well-established best practice), we prefer to compare only SGD with momentum.\n",
        "\n",
        "### Adam\n",
        "While Adam is a powerful adaptive optimizer in many settings, research in computer vision has demonstrated that, for training architectures like ResNet, plain Adam often does not generalize as well as either SGD with momentum or AdamW. AdamW improves upon Adam by decoupling weight decay from the adaptive updates, which typically results in better performance on vision tasks. Therefore, instead of plain Adam, we choose AdamW as the representative adaptive optimizer.\n",
        "\n",
        "---\n",
        "\n",
        "### Experiment 1: Baseline with SGD + Momentum\n",
        "This is the standard baseline configuration for training ResNet18 on CIFAR-10. The combination of SGD with momentum (0.9) is well established for CNNs, and a learning rate of 0.01 is a typical starting point. A weight decay of 5e-4 provides reasonable regularization without overly penalizing the weights.\n",
        "\n",
        "**Settings:**\n",
        "- Optimizer: SGD with momentum\n",
        "- Momentum: 0.9\n",
        "- Learning Rate: 0.01\n",
        "- Weight Decay: 5e-4\n",
        "\n",
        "---\n",
        "\n",
        "### Experiment 2: Increased Weight Decay with SGD + Momentum\n",
        "By increasing the weight decay to 1e-3, this experiment tests whether stronger regularization can help reduce overfitting and improve validation performance, especially if the baseline shows signs of overfitting.\n",
        "\n",
        "**Settings:**\n",
        "- Optimizer: SGD with momentum\n",
        "- Momentum: 0.9\n",
        "- Learning Rate: 0.01\n",
        "- Weight Decay: 1e-3\n",
        "\n",
        "---\n",
        "\n",
        "### Experiment 3: Higher Learning Rate with SGD + Momentum\n",
        "This experiment raises the learning rate to 0.1 to evaluate whether a larger step size can accelerate convergence. However, it will also help determine if the increased learning rate introduces instability or divergence during training.\n",
        "\n",
        "**Settings:**\n",
        "- Optimizer: SGD with momentum\n",
        "- Momentum: 0.9\n",
        "- Learning Rate: 0.1\n",
        "- Weight Decay: 5e-4\n",
        "\n",
        "---\n",
        "\n",
        "### Experiment 4: Using AdamW Optimizer\n",
        "AdamW is an adaptive optimizer that decouples weight decay from the gradient update. It often converges faster and can be more robust to hyperparameter settings. Here, we compare its performance with the SGD + momentum baseline. The typical learning rate for AdamW is lower (1e-3) compared to SGD.\n",
        "\n",
        "**Settings:**\n",
        "- Optimizer: AdamW\n",
        "- Learning Rate: 1e-3\n",
        "- Weight Decay: 5e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "experiment4 = {\n",
        "    \"resnet18_SGDMomentum_baseline\": Arguments(\n",
        "        logdir=\"results/experiment4/resnet18_SGDMomentum_baseline\",\n",
        "        model='resnet18',\n",
        "        model_config='model_configs/resnet18.json',\n",
        "        optimizer='momentum',\n",
        "        lr=0.01,\n",
        "    ),\n",
        "    \"resnet18_SGDMomentum_wd1e-3\": Arguments(\n",
        "        logdir=\"results/experiment4/resnet18_SGDMomentum_wd1e-3\",\n",
        "        model='resnet18',\n",
        "        model_config='model_configs/resnet18.json',\n",
        "        optimizer='momentum',\n",
        "        lr=0.01,\n",
        "        weight_decay=1e-3, # increased weight decay\n",
        "    ),\n",
        "    \"resnet18_SGDMomentum_lr1e-1\": Arguments(\n",
        "        logdir=\"results/experiment4/resnet18_SGDMomentum_lr1e-1\",\n",
        "        model='resnet18',\n",
        "        model_config='model_configs/resnet18.json',\n",
        "        optimizer='momentum',\n",
        "        lr=0.1, # higher learning rate\n",
        "        momentum=0.9,\n",
        "    ),\n",
        "    \"resnet18_AdamW\": Arguments(\n",
        "        logdir=\"results/experiment4/resnet18_AdamW\",\n",
        "        model='resnet18',\n",
        "        model_config='model_configs/resnet18.json',\n",
        "        optimizer='adamw',\n",
        "    ),\n",
        "}\n",
        "\n",
        "run_experiment(experiment4)\n",
        "\n",
        "legend_names = [\"SGDMomentum_baseline\", \"SGDMomentum_wd1e-3\", \"SGDMomentum_lr1e-1\", \"AdamW\"]\n",
        "logdirs = [experiment4[x].logdir for x in experiment4]\n",
        "\n",
        "generate_plots(logdirs, legend_names, \"results/experiment4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!zip -r experiment4.zip results/experiment4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 5\n",
        "\n",
        "Set the patch size to be 4, and find hyper-parameters for your best MLPMixer model. Provide the full hyper-parameters in your report. Visualize the weights (only first layer) of token-mixing MLP in the first block as is described in Figure 5 of the MLPMixer paper. Comment and compare your results with the convolution visualizations. Explain what you think is the reason behind the success of the MLPMixer, especially over normal MLP?\n",
        "\n",
        "---\n",
        "\n",
        "### 1. MLPMixer Patch4 Baseline\n",
        "\n",
        "This experiment is already trained and is the default configuration observed in question 4.4 for patch size 4. It serves as a reference point for all subsequent modifications.\n",
        "\n",
        "**Configuration:**\n",
        "- Optimizer: AdamW  \n",
        "- Learning Rate: 1e-3  \n",
        "- Weight Decay: 5e-4  \n",
        "- Epochs: 15  \n",
        "- Device: CUDA (if available)  \n",
        "- Patch Size: 4  \n",
        "- Embedding Dimension: 256  \n",
        "- Number of Mixer Blocks: 4  \n",
        "- Dropout Rate: 0.0  \n",
        "- Activation: GELU\n",
        "\n",
        "---\n",
        "\n",
        "### 2. MLPMixer Patch4 with Dropout\n",
        "\n",
        "This experiment introduces a dropout rate of 0.1 to the baseline configuration to help regularize the model and reduce overfitting.\n",
        "\n",
        "**Configuration changes:**  \n",
        "- Dropout Rate: 0.1  \n",
        "\n",
        "**Analysis:**  \n",
        "- Examine training vs. validation loss curves for reduced overfitting.  \n",
        "- Look for a narrowing gap between training and validation accuracy.  \n",
        "- Determine if the added dropout improves generalization compared to the baseline.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. MLPMixer Patch4 with Embedding Dimension 512 (Increased Embedding Experiment)\n",
        "\n",
        "This experiment increases the embedding dimension from 256 to 512, aiming to capture richer representations for each patch.\n",
        "\n",
        "**Configuration changes:**  \n",
        "- Embedding Dimension: 512  \n",
        "\n",
        "**Analysis:**  \n",
        "- Evaluate whether the higher capacity leads to improved validation accuracy.  \n",
        "- Monitor for potential overfitting due to the increased number of parameters.  \n",
        "- Compare feature quality and training dynamics against the baseline.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. MLPMixer Patch4 with 8 Mixer Blocks (Deeper Model Experiment)\n",
        "\n",
        "This experiment doubles the number of MixerBlocks from 4 to 8 while keeping the embedding dimension at 256. The goal is to assess if additional depth improves the mixing of token information.\n",
        "\n",
        "**Configuration changes:**\n",
        "- Number of Mixer Blocks: 8  \n",
        "\n",
        "**Analysis:**  \n",
        "- Investigate improvements in capturing higher-level interactions between patches.  \n",
        "- Observe the impact on training time and whether deeper mixing leads to better validation performance.  \n",
        "- Look for signs of overfitting as the model depth increases.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. MLPMixer Patch4 with Increased Capacity and Dropout\n",
        "\n",
        "This experiment combines the modifications from Experiments 3 and 4 by increasing both the embedding dimension to 512 and the number of MixerBlocks to 8, while also adding a dropout rate of 0.1. This tests the upper bound of capacity while controlling overfitting.\n",
        "\n",
        "**Configuration changes:**\n",
        "- Embedding Dimension: 512  \n",
        "- Number of Mixer Blocks: 8  \n",
        "\n",
        "**Analysis:**  \n",
        "- Compare the training and validation performance with all other experiments.  \n",
        "- Assess whether the combined increases in capacity and depth, along with dropout, yield the best generalization.  \n",
        "- Evaluate the token-mixing MLP visualizations (if available) to see if the model learns richer spatial interactions.  \n",
        "- Use these insights to decide if the extra complexity is justified by improved performance.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "experiment5 = {\n",
        "    \"mlpmixer_dropout\": Arguments(\n",
        "        logdir=\"results/experiment5/mlpmixer_dropout\",\n",
        "        model='mlpmixer',\n",
        "        model_config='model_configs/mlpmixer/mlpmixer_patch4_dropout.json',\n",
        "    ),\n",
        "    \"mlpmixer_embed512\": Arguments(\n",
        "        logdir=\"results/experiment5/mlpmixer_embed512\",\n",
        "        model='mlpmixer',\n",
        "        model_config='model_configs/mlpmixer/mlpmixer_patch4_embed512.json',\n",
        "    ),\n",
        "    \"mlpmixer_deep\": Arguments(\n",
        "        logdir=\"results/experiment5/mlpmixer_deep\",\n",
        "        model='mlpmixer',\n",
        "        model_config='model_configs/mlpmixer/mlpmixer_patch4_deep.json',\n",
        "    ),\n",
        "    \"mlpmixer_combined\": Arguments(\n",
        "        logdir=\"results/experiment5/mlpmixer_combined\",\n",
        "        model='mlpmixer',\n",
        "        model_config='model_configs/mlpmixer/mlpmixer_patch4_combined.json',\n",
        "    )\n",
        "}\n",
        "\n",
        "run_experiment(experiment5)\n",
        "\n",
        "# Since the baseline experiment was already done in experiment 3, just copy the results to experiment 5 results\n",
        "shutil.copytree(\"results/experiment3/mlpmixer_patch4\", \"results/experiment5/mlpmixer_patch4_baseline\")\n",
        "\n",
        "legend_names = [\"Baseline\", \"Dropout\", \"Embedding 512\", \"Deep\", \"Combined\"]\n",
        "logdirs = [\"results/experiment5/mlpmixer_patch4_baseline\"] + [experiment5[x].logdir for x in experiment5]\n",
        "\n",
        "generate_plots(logdirs, legend_names, \"results/experiment5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!zip -r experiment5.zip results/experiment5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 6\n",
        "\n",
        "This experiment investigates how the allocation of capacity between the token-mixing and channel-mixing MLPs affects performance. We vary the mlp_ratio parameter, which controls the hidden dimensions of each MLP relative to the embedding dimension, to see how different balances influence learning dynamics and generalization on CIFAR-10.\n",
        "\n",
        "By comparing these configurations, we will gain insight into how different capacity distributions between token mixing and channel mixing affect the model's ability to learn both spatial and feature representations. I will analyze the loss and accuracy curves for each experiment, and consider any qualitative visualizations (e.g., weight heatmaps) to support my conclusions.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Baseline (Default)\n",
        "\n",
        "This configuration uses the default asymmetric mlp_ratio values on a pre-trained model from question 4.4.\n",
        "\n",
        "**Configuration:**\n",
        "- **Patch Size:** 4 (yielding an 8×8 grid for a 32×32 image)\n",
        "- **Embedding Dimension:** 256\n",
        "- **Number of MixerBlocks:** 4\n",
        "- **Dropout Rate:** 0.1\n",
        "- **Activation Function:** GELU\n",
        "- **mlp_ratio:** [0.5, 4.0]\n",
        "  - *Token-Mixing hidden size:* 0.5 × 512 = 256  \n",
        "  - *Channel-Mixing hidden size:* 4.0 × 512 = 2048\n",
        "\n",
        "**Analysis:**\n",
        "- Use this configuration as the reference baseline.\n",
        "- Examine training and validation loss/accuracy curves.\n",
        "- Assess if the model achieves a good balance between underfitting and overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Increased Token-Mixing Capacity\n",
        "\n",
        "This configuration increases the capacity of the token-mixing MLP, aiming to better capture spatial (inter-patch) relationships while keeping channel-mixing capacity unchanged.\n",
        "\n",
        "**Configuration:**\n",
        "- **mlp_ratio:** [1.0, 4.0]\n",
        "  - *Token-Mixing hidden size:* 1.0 × 512 = 512  \n",
        "  - *Channel-Mixing hidden size:* 4.0 × 512 = 2048\n",
        "\n",
        "**Analysis:**\n",
        "- Determine if enhancing token mixing improves spatial information aggregation.\n",
        "- Compare validation performance against the baseline.\n",
        "- Look for improvements in generalization or signs of overfitting (e.g., a large gap between training and validation metrics).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Increased Channel-Mixing Capacity\n",
        "\n",
        "This configuration boosts the capacity of the channel-mixing MLP to enable richer intra-token feature transformation, potentially leading to better feature representations.\n",
        "\n",
        "**Configuration:**\n",
        "- **mlp_ratio:** [0.5, 8.0]\n",
        "  - *Token-Mixing hidden size:* 0.5 × 512 = 256  \n",
        "  - *Channel-Mixing hidden size:* 8.0 × 512 = 4096\n",
        "\n",
        "**Analysis:**\n",
        "- Evaluate whether increasing channel mixing improves intra-patch feature extraction.\n",
        "- Check for higher training accuracy accompanied by overfitting.\n",
        "- Examine if the enhanced channel capacity translates to improved validation accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Balanced Capacity for Both Mixings\n",
        "\n",
        "This configuration allocates equal capacity to both token and channel mixing, challenging the assumption of asymmetry by treating spatial and feature processing equally.\n",
        "\n",
        "**Configuration:**\n",
        "- **mlp_ratio:** [1.0, 1.0]\n",
        "  - *Token-Mixing hidden size:* 1.0 × 512 = 512  \n",
        "  - *Channel-Mixing hidden size:* 1.0 × 512 = 512\n",
        "\n",
        "**Analysis:**\n",
        "- Assess whether equal capacity for token and channel mixing yields a more balanced performance.\n",
        "- Compare training and validation metrics with the asymmetric configurations.\n",
        "- Determine if the balanced design leads to stable training and improved generalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "experiment6 = {\n",
        "    \"mlpmixer_increased_token\": Arguments(\n",
        "        logdir=\"results/experiment6/mlpmixer_increased_token\",\n",
        "        model='mlpmixer',\n",
        "        model_config='model_configs/mlpmixer/mlpmixer_increased_token.json',\n",
        "    ),\n",
        "    \"mlpmixer_increased_channel\": Arguments(\n",
        "        logdir=\"results/experiment6/mlpmixer_increased_channel\",\n",
        "        model='mlpmixer',\n",
        "        model_config='model_configs/mlpmixer/mlpmixer_increased_channel.json',\n",
        "    ),\n",
        "    \"mlpmixer_balanced_ratio\": Arguments(\n",
        "        logdir=\"results/experiment6/mlpmixer_balanced_ratio\",\n",
        "        model='mlpmixer',\n",
        "        model_config='model_configs/mlpmixer/mlpmixer_balanced_ratio.json',\n",
        "    )\n",
        "}\n",
        "\n",
        "run_experiment(experiment6)\n",
        "\n",
        "# Since the baseline experiment was already done in experiment 3, just copy the results to experiment 6 results\n",
        "shutil.copytree(\"results/experiment3/mlpmixer_patch4\", \"results/experiment6/mlpmixer_baseline\")\n",
        "\n",
        "legend_names = [\"Baseline\", \"Increased Token\", \"Increased Channel\", \"Balanced Ratio\"]\n",
        "logdirs = [\"results/experiment6/mlpmixer_baseline\"] + [experiment6[x].logdir for x in experiment6]\n",
        "\n",
        "generate_plots(logdirs, legend_names, f\"results/experiment6\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!zip -r experiment6.zip results/experiment6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 7\n",
        "\n",
        "Compare the gradient flow (e.g., norms of gradients at different layers) during backpropagation for all three architectures (MLP, ResNet18 and MLPMixer). Analyze and compare the behavior of gradients in each architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "experiment7 = {}\n",
        "\n",
        "run_experiment(experiment7)\n",
        "\n",
        "legend_names = []\n",
        "logdirs = []\n",
        "\n",
        "generate_plots(logdirs, legend_names, f\"results/experiment7\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!zip -r experiment7.zip results/experiment7"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "ift",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
