Okay now let's move on to the next question X. Here is more information about the experiment for question X:

Here is the configuration of the experiments of the question:
{
    "mlpmixer_baseline": {
        "batch_size": 128,
        "model": "mlpmixer",
        "model_config": "model_configs/mlpmixer/mlpmixer_patch4.json",
        "optimizer": "adamw",
        "epochs": 15,
        "lr": 0.001,
        "momentum": 0.9,
        "weight_decay": 0.0005,
        "logdir": "results/experiment3/mlpmixer_patch4",
        "seed": 42,
        "device": "cuda",
        "visualize": false,
        "print_every": 80
    },
    "mlpmixer_increased_token": {
        "batch_size": 128,
        "model": "mlpmixer",
        "model_config": "model_configs/mlpmixer/mlpmixer_increased_token.json",
        "optimizer": "adamw",
        "epochs": 15,
        "lr": 0.001,
        "momentum": 0.9,
        "weight_decay": 0.0005,
        "logdir": "results/experiment6/mlpmixer_increased_token",
        "seed": 42,
        "device": "cuda",
        "visualize": false,
        "print_every": 80
    },
    "mlpmixer_increased_channel": {
        "batch_size": 128,
        "model": "mlpmixer",
        "model_config": "model_configs/mlpmixer/mlpmixer_increased_channel.json",
        "optimizer": "adamw",
        "epochs": 15,
        "lr": 0.001,
        "momentum": 0.9,
        "weight_decay": 0.0005,
        "logdir": "results/experiment6/mlpmixer_increased_channel",
        "seed": 42,
        "device": "cuda",
        "visualize": false,
        "print_every": 80
    },
    "mlpmixer_balanced_ratio": {
        "batch_size": 128,
        "model": "mlpmixer",
        "model_config": "model_configs/mlpmixer/mlpmixer_balanced_ratio.json",
        "optimizer": "adamw",
        "epochs": 15,
        "lr": 0.001,
        "momentum": 0.9,
        "weight_decay": 0.0005,
        "logdir": "results/experiment6/mlpmixer_balanced_ratio",
        "seed": 42,
        "device": "cuda",
        "visualize": false,
        "print_every": 80
    }
}

Here is the models configuration:
{
    "mlpmixer_baseline": {
        "num_classes": 10,
        "img_size": 32,
        "patch_size": 4,
        "embed_dim": 256,
        "num_blocks": 4,
        "drop_rate": 0.0,
        "activation": "gelu",
        "mlp_ratio": [
            0.5,
            4.0
        ]
    },
    "mlpmixer_increased_token": {
        "num_classes": 10,
        "img_size": 32,
        "patch_size": 4,
        "embed_dim": 256,
        "num_blocks": 4,
        "drop_rate": 0.0,
        "activation": "gelu",
        "mlp_ratio": [
            1.0,
            4.0
        ]
    },
    "mlpmixer_increased_channel": {
        "num_classes": 10,
        "img_size": 32,
        "patch_size": 4,
        "embed_dim": 256,
        "num_blocks": 4,
        "drop_rate": 0.0,
        "activation": "gelu",
        "mlp_ratio": [
            0.5,
            8.0
        ]
    },
    "mlpmixer_balanced_ratio": {
        "num_classes": 10,
        "img_size": 32,
        "patch_size": 4,
        "embed_dim": 256,
        "num_blocks": 4,
        "drop_rate": 0.0,
        "activation": "gelu",
        "mlp_ratio": [
            1,
            1
        ]
    }
}

Here is the raw results of the training process:
{
    "mlpmixer_baseline": {
        "train_losses": [
            1.59056757488142,
            1.2227207038477286,
            1.0673863673821482,
            0.9666666777385259,
            0.8902156926967479,
            0.8265206530223209,
            0.7667353297570492,
            0.720503978070371,
            0.677208404001008,
            0.6341182035937947,
            0.5930588315864574,
            0.5570061399556296,
            0.5190531995221758,
            0.48049120787541777,
            0.43761973902370827,
            0.40158312379295014,
            0.37333310049483565,
            0.33518435065223284,
            0.30604451419281475,
            0.27773866660234947,
            0.2519851529020871,
            0.23473695175260564,
            0.21311099385773682,
            0.19976370826236195,
            0.18306460842872274,
            0.16750121668533038,
            0.16205996006430043,
            0.14354832617942773,
            0.14699117406311216,
            0.12961057774149465
        ],
        "valid_losses": [
            1.3177850663661956,
            1.1008550748229025,
            0.9986695647239685,
            0.8782955825328828,
            0.8843006238341333,
            0.8059065461158751,
            0.7303763478994367,
            0.669857856631279,
            0.637188395857811,
            0.5940837830305099,
            0.5329302720725536,
            0.4984251067042352,
            0.4734350807964803,
            0.4290427394211292,
            0.3806643977761269,
            0.3661802306771279,
            0.3120447274297477,
            0.3303615685552359,
            0.30074929930269717,
            0.27705284394323826,
            0.23263729922473433,
            0.2310401747003198,
            0.19616048000752928,
            0.21852249531075357,
            0.196317803603597,
            0.21573120038956406,
            0.17683613132685427,
            0.15450892911758277,
            0.17804569378495216,
            0.1780174387618899
        ],
        "train_accs": [
            0.4206508190883191,
            0.5614761396011396,
            0.6197471509971508,
            0.6561832264957266,
            0.6860532407407403,
            0.7081330128205128,
            0.7293892450142445,
            0.7461939102564097,
            0.7594818376068371,
            0.7759526353276346,
            0.7892628205128197,
            0.8013488247863242,
            0.8157941595441586,
            0.8295717592592585,
            0.844484508547008,
            0.8550792378917373,
            0.8672765313390312,
            0.8807425213675214,
            0.8897569444444446,
            0.9012197293447295,
            0.9105902777777763,
            0.9157095797720787,
            0.9236778846153826,
            0.9297320156695148,
            0.935340990028488,
            0.9408609330484321,
            0.9426860754985752,
            0.9498530982905988,
            0.9486289173789181,
            0.9548165954415971
        ],
        "valid_accs": [
            0.522265625,
            0.6099609375000001,
            0.6494140625,
            0.6904296874999999,
            0.691796875,
            0.7195312500000001,
            0.7455078124999999,
            0.7714843750000002,
            0.7787109375,
            0.793359375,
            0.8173828125,
            0.8306640624999999,
            0.8328124999999998,
            0.8439453124999999,
            0.8644531250000003,
            0.8685546874999999,
            0.88828125,
            0.8880859374999999,
            0.8951171874999999,
            0.9048828125,
            0.9259765625000002,
            0.9253906250000001,
            0.9388671874999999,
            0.9279296874999998,
            0.9388671874999999,
            0.9343750000000002,
            0.9476562500000002,
            0.9544921875000001,
            0.9488281250000001,
            0.9519531249999998
        ],
        "test_loss": 0.99359352000152,
        "test_acc": 0.7594936708860762
    },
    "mlpmixer_increased_token": {
        "train_losses": [
            1.5943384445630582,
            1.2209880698440423,
            1.076570908058743,
            0.9735304023805167,
            0.887194606653305,
            0.8158609829736911,
            0.7632129752737841,
            0.7084442484922214,
            0.6707433872243274,
            0.628301247572288,
            0.584596789970017,
            0.546049919393327,
            0.5050637243140457,
            0.46296230901000845,
            0.42804102871322897,
            0.39411646658368926,
            0.3515965513609069,
            0.32457283351496075,
            0.29823780497084995,
            0.26833606891312833,
            0.24894734412601516,
            0.22167651905038757,
            0.2021286876665222,
            0.19089760251364477,
            0.17546130591563008,
            0.16247656927616508,
            0.15984600610457944,
            0.14063252235876872,
            0.1353462474723147,
            0.1310798382742112
        ],
        "valid_losses": [
            1.3048694103956227,
            1.107568010687828,
            0.9590192973613738,
            0.9426962777972221,
            0.8404218122363091,
            0.7271403834223749,
            0.7237005300819872,
            0.688840802013874,
            0.6269572600722314,
            0.5942946463823319,
            0.5158172644674778,
            0.49803412929177276,
            0.4558470800518989,
            0.40252851843833926,
            0.38203416541218765,
            0.34816753156483177,
            0.31110544539988044,
            0.3193626001477241,
            0.26213968750089406,
            0.2500332063063979,
            0.2367550190538168,
            0.20763159394264225,
            0.22774833049625162,
            0.22858998645097017,
            0.20757829640060663,
            0.19211707450449464,
            0.17231497988104824,
            0.1771123379468918,
            0.19017578121274709,
            0.17483040667138988
        ],
        "train_accs": [
            0.4188701923076924,
            0.559139066951567,
            0.6163862179487175,
            0.6560051638176638,
            0.6870325854700849,
            0.7115162037037038,
            0.7315927706552698,
            0.7485087250712245,
            0.7626201923076914,
            0.7746394230769227,
            0.7932469729344723,
            0.8052216880341871,
            0.8209579772079756,
            0.8342459045584033,
            0.850338319088318,
            0.8607104700854697,
            0.8749999999999998,
            0.8839031339031346,
            0.893295940170941,
            0.9054709757834761,
            0.9117922008547009,
            0.9209624287749284,
            0.9284633190883179,
            0.9316239316239302,
            0.9369880698005686,
            0.9425747863247853,
            0.9433092948717945,
            0.9506098646723649,
            0.9520566239316246,
            0.9532140313390326
        ],
        "valid_accs": [
            0.53671875,
            0.6105468749999998,
            0.6615234375000001,
            0.6792968749999999,
            0.7099609375000002,
            0.7503906250000002,
            0.7533203125,
            0.7634765625,
            0.7802734375000002,
            0.7900390624999999,
            0.8251953125000001,
            0.8199218750000001,
            0.8357421875000002,
            0.8570312500000002,
            0.8705078124999998,
            0.8791015625,
            0.8935546875000001,
            0.8845703125,
            0.91328125,
            0.9162109375000002,
            0.92109375,
            0.9363281250000002,
            0.9292968749999998,
            0.9291015625000001,
            0.9353515625,
            0.9417968750000001,
            0.95234375,
            0.9490234374999997,
            0.9455078125,
            0.9544921875
        ],
        "test_loss": 0.9684725516959082,
        "test_acc": 0.7721518987341776
    },
    "mlpmixer_increased_channel": {
        "train_losses": [
            1.6207627598037064,
            1.2537739334622677,
            1.1007896845157326,
            0.9795179569143855,
            0.8963028262143796,
            0.8276889989858337,
            0.7738724238852156,
            0.7160016145461642,
            0.6670184511574587,
            0.6228884203821169,
            0.5785908384880116,
            0.5363729495947859,
            0.4920691204206896,
            0.44976718103953567,
            0.40975580264700096,
            0.36726300401395556,
            0.33766027936908266,
            0.293744517506188,
            0.2646662898552724,
            0.23196356807254323,
            0.21832286296916475,
            0.19145735230116426,
            0.1752031809753843,
            0.1699977257312873,
            0.1476958366561988,
            0.13972059984365076,
            0.12827837595615296,
            0.12755530285105077,
            0.118805126597484,
            0.10962080193465928
        ],
        "valid_losses": [
            1.3599585175514224,
            1.1628887012600897,
            1.0251869067549706,
            0.9419319555163382,
            0.8847134456038478,
            0.8175906315445899,
            0.7076193481683731,
            0.6620796620845795,
            0.5907293766736984,
            0.6211093015968798,
            0.5899010114371777,
            0.4944046124815942,
            0.47416120991110805,
            0.3708438068628311,
            0.3553249806165696,
            0.3539311740547419,
            0.3290877953171729,
            0.25302613605745133,
            0.2558613166213035,
            0.23355314694345003,
            0.23021244183182718,
            0.21251543592661623,
            0.19663942959159614,
            0.21149746226146812,
            0.184724819555413,
            0.16821932888124141,
            0.20453405324369667,
            0.1633456589654088,
            0.18372185677289965,
            0.16920256600133152
        ],
        "train_accs": [
            0.4049367877492874,
            0.549123041310541,
            0.6071714743589741,
            0.657652243589743,
            0.6836493945868944,
            0.708021723646723,
            0.7266070156695155,
            0.7470842236467233,
            0.7659588675213673,
            0.7785345441595442,
            0.7934695512820502,
            0.8090055199430188,
            0.8253427706552698,
            0.8421474358974346,
            0.8548789173789174,
            0.868678774928775,
            0.8790064102564104,
            0.8938078703703712,
            0.9051148504273513,
            0.9174011752136744,
            0.9237446581196563,
            0.9320913461538449,
            0.9389022435897426,
            0.939814814814814,
            0.9468705484330484,
            0.9510995370370383,
            0.9548388532763546,
            0.9553062678062695,
            0.9583555911680938,
            0.9624955484330517
        ],
        "valid_accs": [
            0.5158203125,
            0.5781249999999999,
            0.642578125,
            0.6578125000000001,
            0.6892578125000001,
            0.7152343750000001,
            0.7650390625000001,
            0.767578125,
            0.7904296875000002,
            0.7792968750000001,
            0.7874999999999999,
            0.8267578124999999,
            0.8412109374999999,
            0.8689453125000001,
            0.8744140624999998,
            0.8779296875,
            0.8900390625,
            0.91875,
            0.9181640625000003,
            0.9259765625000002,
            0.9234374999999998,
            0.9351562499999999,
            0.9337890624999999,
            0.9312500000000001,
            0.9468749999999999,
            0.9525390625,
            0.9417968749999999,
            0.9529296875000001,
            0.9501953125000001,
            0.9513671875
        ],
        "test_loss": 1.010584188413016,
        "test_acc": 0.7690862341772151
    },
    "mlpmixer_balanced_ratio": {
        "train_losses": [
            1.540096066955827,
            1.1851333595409013,
            1.0430627945821167,
            0.9448349372274173,
            0.8758508661533687,
            0.8193545781309448,
            0.7702689678580671,
            0.7261628876214691,
            0.6877864880269752,
            0.6533636112838048,
            0.6278960967165786,
            0.5948328974410002,
            0.5638024949619918,
            0.5348932353349837,
            0.5096108563605196,
            0.48392861937185977,
            0.4582369188722384,
            0.43487523305110437,
            0.41281222876829976,
            0.38571591114896003,
            0.3695021495435312,
            0.3526590337865373,
            0.3333974770586036,
            0.3159448751527018,
            0.2903148936070608,
            0.27872393447958854,
            0.2674120805329746,
            0.2507225124966723,
            0.23457649547574863,
            0.2222875818992274
        ],
        "valid_losses": [
            1.2488197028636931,
            1.0866358041763307,
            0.9284276545047758,
            0.881082910299301,
            0.849552671611309,
            0.756802822649479,
            0.7563488930463792,
            0.6716789275407792,
            0.6256226800382139,
            0.5982558511197567,
            0.607153172045946,
            0.5479929760098458,
            0.5194308668375015,
            0.46664038896560656,
            0.44309033751487736,
            0.4636443085968494,
            0.4224999029189349,
            0.39369439631700526,
            0.37185541987419135,
            0.3511482074856758,
            0.3278797589242457,
            0.33649170435965065,
            0.32803599834442143,
            0.2814156144857407,
            0.28631314262747753,
            0.28505730368196963,
            0.2737358876038342,
            0.2616728845983744,
            0.2695825286209583,
            0.23935665804892775
        ],
        "train_accs": [
            0.4349848646723648,
            0.5756098646723647,
            0.6286725427350432,
            0.6635728276353272,
            0.6901041666666664,
            0.7110265313390305,
            0.727764423076922,
            0.742922008547008,
            0.755408653846153,
            0.7699207621082627,
            0.7786013176638172,
            0.7886396011396007,
            0.8011930199430193,
            0.8112090455840443,
            0.8181089743589726,
            0.8279469373219366,
            0.8374287749287744,
            0.8451522435897431,
            0.8521189458689455,
            0.8620681980056976,
            0.8686787749287749,
            0.8745325854700856,
            0.8791844729344734,
            0.8876424501424515,
            0.8963897792022786,
            0.9013310185185186,
            0.9037126068376071,
            0.9098780270655259,
            0.9150195868945862,
            0.9208066239316228
        ],
        "valid_accs": [
            0.5591796875,
            0.6095703124999999,
            0.6691406249999999,
            0.6849609374999998,
            0.7009765625,
            0.7337890625,
            0.7390625000000001,
            0.7701171875000002,
            0.7769531250000001,
            0.7898437500000001,
            0.7900390625,
            0.8076171874999999,
            0.8189453125000001,
            0.8308593750000002,
            0.8470703125000001,
            0.83359375,
            0.8531249999999999,
            0.8574218749999998,
            0.8730468750000001,
            0.8767578125000001,
            0.8876953124999998,
            0.886328125,
            0.8904296875000002,
            0.9009765624999999,
            0.902734375,
            0.9015625000000002,
            0.909765625,
            0.9132812499999998,
            0.9134765625,
            0.9201171875
        ],
        "test_loss": 0.8979068851169146,
        "test_acc": 0.7569224683544302
    }
}

Here is the summary of every model architecture:
### mlpmixer_baseline

MLPMIXER CONFIGURATION
----------------------------------------------------------------
num_classes:	10
img_size:	32
patch_size:	4
embed_dim:	256
num_blocks:	4
drop_rate:	0.0
activation:	gelu
mlp_ratio:	[0.5, 4.0]
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 256, 8, 8]          12,544
        PatchEmbed-2              [-1, 64, 256]               0
         LayerNorm-3              [-1, 64, 256]             512
            Linear-4             [-1, 256, 128]           8,320
              GELU-5             [-1, 256, 128]               0
           Dropout-6             [-1, 256, 128]               0
            Linear-7              [-1, 256, 64]           8,256
           Dropout-8              [-1, 256, 64]               0
               Mlp-9              [-1, 256, 64]               0
        LayerNorm-10              [-1, 64, 256]             512
           Linear-11             [-1, 64, 1024]         263,168
             GELU-12             [-1, 64, 1024]               0
          Dropout-13             [-1, 64, 1024]               0
           Linear-14              [-1, 64, 256]         262,400
          Dropout-15              [-1, 64, 256]               0
              Mlp-16              [-1, 64, 256]               0
       MixerBlock-17              [-1, 64, 256]               0
        LayerNorm-18              [-1, 64, 256]             512
           Linear-19             [-1, 256, 128]           8,320
             GELU-20             [-1, 256, 128]               0
          Dropout-21             [-1, 256, 128]               0
           Linear-22              [-1, 256, 64]           8,256
          Dropout-23              [-1, 256, 64]               0
              Mlp-24              [-1, 256, 64]               0
        LayerNorm-25              [-1, 64, 256]             512
           Linear-26             [-1, 64, 1024]         263,168
             GELU-27             [-1, 64, 1024]               0
          Dropout-28             [-1, 64, 1024]               0
           Linear-29              [-1, 64, 256]         262,400
          Dropout-30              [-1, 64, 256]               0
              Mlp-31              [-1, 64, 256]               0
       MixerBlock-32              [-1, 64, 256]               0
        LayerNorm-33              [-1, 64, 256]             512
           Linear-34             [-1, 256, 128]           8,320
             GELU-35             [-1, 256, 128]               0
          Dropout-36             [-1, 256, 128]               0
           Linear-37              [-1, 256, 64]           8,256
          Dropout-38              [-1, 256, 64]               0
              Mlp-39              [-1, 256, 64]               0
        LayerNorm-40              [-1, 64, 256]             512
           Linear-41             [-1, 64, 1024]         263,168
             GELU-42             [-1, 64, 1024]               0
          Dropout-43             [-1, 64, 1024]               0
           Linear-44              [-1, 64, 256]         262,400
          Dropout-45              [-1, 64, 256]               0
              Mlp-46              [-1, 64, 256]               0
       MixerBlock-47              [-1, 64, 256]               0
        LayerNorm-48              [-1, 64, 256]             512
           Linear-49             [-1, 256, 128]           8,320
             GELU-50             [-1, 256, 128]               0
          Dropout-51             [-1, 256, 128]               0
           Linear-52              [-1, 256, 64]           8,256
          Dropout-53              [-1, 256, 64]               0
              Mlp-54              [-1, 256, 64]               0
        LayerNorm-55              [-1, 64, 256]             512
           Linear-56             [-1, 64, 1024]         263,168
             GELU-57             [-1, 64, 1024]               0
          Dropout-58             [-1, 64, 1024]               0
           Linear-59              [-1, 64, 256]         262,400
          Dropout-60              [-1, 64, 256]               0
              Mlp-61              [-1, 64, 256]               0
       MixerBlock-62              [-1, 64, 256]               0
        LayerNorm-63              [-1, 64, 256]             512
           Linear-64                   [-1, 10]           2,570
================================================================
Total params: 2,188,298
Trainable params: 2,188,298
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 13.88
Params size (MB): 8.35
Estimated Total Size (MB): 22.23
----------------------------------------------------------------


### mlpmixer_increased_token

MLPMIXER CONFIGURATION
----------------------------------------------------------------
num_classes:	10
img_size:	32
patch_size:	4
embed_dim:	256
num_blocks:	4
drop_rate:	0.0
activation:	gelu
mlp_ratio:	[1.0, 4.0]
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 256, 8, 8]          12,544
        PatchEmbed-2              [-1, 64, 256]               0
         LayerNorm-3              [-1, 64, 256]             512
            Linear-4             [-1, 256, 256]          16,640
              GELU-5             [-1, 256, 256]               0
           Dropout-6             [-1, 256, 256]               0
            Linear-7              [-1, 256, 64]          16,448
           Dropout-8              [-1, 256, 64]               0
               Mlp-9              [-1, 256, 64]               0
        LayerNorm-10              [-1, 64, 256]             512
           Linear-11             [-1, 64, 1024]         263,168
             GELU-12             [-1, 64, 1024]               0
          Dropout-13             [-1, 64, 1024]               0
           Linear-14              [-1, 64, 256]         262,400
          Dropout-15              [-1, 64, 256]               0
              Mlp-16              [-1, 64, 256]               0
       MixerBlock-17              [-1, 64, 256]               0
        LayerNorm-18              [-1, 64, 256]             512
           Linear-19             [-1, 256, 256]          16,640
             GELU-20             [-1, 256, 256]               0
          Dropout-21             [-1, 256, 256]               0
           Linear-22              [-1, 256, 64]          16,448
          Dropout-23              [-1, 256, 64]               0
              Mlp-24              [-1, 256, 64]               0
        LayerNorm-25              [-1, 64, 256]             512
           Linear-26             [-1, 64, 1024]         263,168
             GELU-27             [-1, 64, 1024]               0
          Dropout-28             [-1, 64, 1024]               0
           Linear-29              [-1, 64, 256]         262,400
          Dropout-30              [-1, 64, 256]               0
              Mlp-31              [-1, 64, 256]               0
       MixerBlock-32              [-1, 64, 256]               0
        LayerNorm-33              [-1, 64, 256]             512
           Linear-34             [-1, 256, 256]          16,640
             GELU-35             [-1, 256, 256]               0
          Dropout-36             [-1, 256, 256]               0
           Linear-37              [-1, 256, 64]          16,448
          Dropout-38              [-1, 256, 64]               0
              Mlp-39              [-1, 256, 64]               0
        LayerNorm-40              [-1, 64, 256]             512
           Linear-41             [-1, 64, 1024]         263,168
             GELU-42             [-1, 64, 1024]               0
          Dropout-43             [-1, 64, 1024]               0
           Linear-44              [-1, 64, 256]         262,400
          Dropout-45              [-1, 64, 256]               0
              Mlp-46              [-1, 64, 256]               0
       MixerBlock-47              [-1, 64, 256]               0
        LayerNorm-48              [-1, 64, 256]             512
           Linear-49             [-1, 256, 256]          16,640
             GELU-50             [-1, 256, 256]               0
          Dropout-51             [-1, 256, 256]               0
           Linear-52              [-1, 256, 64]          16,448
          Dropout-53              [-1, 256, 64]               0
              Mlp-54              [-1, 256, 64]               0
        LayerNorm-55              [-1, 64, 256]             512
           Linear-56             [-1, 64, 1024]         263,168
             GELU-57             [-1, 64, 1024]               0
          Dropout-58             [-1, 64, 1024]               0
           Linear-59              [-1, 64, 256]         262,400
          Dropout-60              [-1, 64, 256]               0
              Mlp-61              [-1, 64, 256]               0
       MixerBlock-62              [-1, 64, 256]               0
        LayerNorm-63              [-1, 64, 256]             512
           Linear-64                   [-1, 10]           2,570
================================================================
Total params: 2,254,346
Trainable params: 2,254,346
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 16.88
Params size (MB): 8.60
Estimated Total Size (MB): 25.49
----------------------------------------------------------------


### mlpmixer_increased_channel

MLPMIXER CONFIGURATION
----------------------------------------------------------------
num_classes:	10
img_size:	32
patch_size:	4
embed_dim:	256
num_blocks:	4
drop_rate:	0.0
activation:	gelu
mlp_ratio:	[0.5, 8.0]
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 256, 8, 8]          12,544
        PatchEmbed-2              [-1, 64, 256]               0
         LayerNorm-3              [-1, 64, 256]             512
            Linear-4             [-1, 256, 128]           8,320
              GELU-5             [-1, 256, 128]               0
           Dropout-6             [-1, 256, 128]               0
            Linear-7              [-1, 256, 64]           8,256
           Dropout-8              [-1, 256, 64]               0
               Mlp-9              [-1, 256, 64]               0
        LayerNorm-10              [-1, 64, 256]             512
           Linear-11             [-1, 64, 2048]         526,336
             GELU-12             [-1, 64, 2048]               0
          Dropout-13             [-1, 64, 2048]               0
           Linear-14              [-1, 64, 256]         524,544
          Dropout-15              [-1, 64, 256]               0
              Mlp-16              [-1, 64, 256]               0
       MixerBlock-17              [-1, 64, 256]               0
        LayerNorm-18              [-1, 64, 256]             512
           Linear-19             [-1, 256, 128]           8,320
             GELU-20             [-1, 256, 128]               0
          Dropout-21             [-1, 256, 128]               0
           Linear-22              [-1, 256, 64]           8,256
          Dropout-23              [-1, 256, 64]               0
              Mlp-24              [-1, 256, 64]               0
        LayerNorm-25              [-1, 64, 256]             512
           Linear-26             [-1, 64, 2048]         526,336
             GELU-27             [-1, 64, 2048]               0
          Dropout-28             [-1, 64, 2048]               0
           Linear-29              [-1, 64, 256]         524,544
          Dropout-30              [-1, 64, 256]               0
              Mlp-31              [-1, 64, 256]               0
       MixerBlock-32              [-1, 64, 256]               0
        LayerNorm-33              [-1, 64, 256]             512
           Linear-34             [-1, 256, 128]           8,320
             GELU-35             [-1, 256, 128]               0
          Dropout-36             [-1, 256, 128]               0
           Linear-37              [-1, 256, 64]           8,256
          Dropout-38              [-1, 256, 64]               0
              Mlp-39              [-1, 256, 64]               0
        LayerNorm-40              [-1, 64, 256]             512
           Linear-41             [-1, 64, 2048]         526,336
             GELU-42             [-1, 64, 2048]               0
          Dropout-43             [-1, 64, 2048]               0
           Linear-44              [-1, 64, 256]         524,544
          Dropout-45              [-1, 64, 256]               0
              Mlp-46              [-1, 64, 256]               0
       MixerBlock-47              [-1, 64, 256]               0
        LayerNorm-48              [-1, 64, 256]             512
           Linear-49             [-1, 256, 128]           8,320
             GELU-50             [-1, 256, 128]               0
          Dropout-51             [-1, 256, 128]               0
           Linear-52              [-1, 256, 64]           8,256
          Dropout-53              [-1, 256, 64]               0
              Mlp-54              [-1, 256, 64]               0
        LayerNorm-55              [-1, 64, 256]             512
           Linear-56             [-1, 64, 2048]         526,336
             GELU-57             [-1, 64, 2048]               0
          Dropout-58             [-1, 64, 2048]               0
           Linear-59              [-1, 64, 256]         524,544
          Dropout-60              [-1, 64, 256]               0
              Mlp-61              [-1, 64, 256]               0
       MixerBlock-62              [-1, 64, 256]               0
        LayerNorm-63              [-1, 64, 256]             512
           Linear-64                   [-1, 10]           2,570
================================================================
Total params: 4,289,546
Trainable params: 4,289,546
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 19.88
Params size (MB): 16.36
Estimated Total Size (MB): 36.25
----------------------------------------------------------------


### mlpmixer_balanced_ratio

MLPMIXER CONFIGURATION
----------------------------------------------------------------
num_classes:	10
img_size:	32
patch_size:	4
embed_dim:	256
num_blocks:	4
drop_rate:	0.0
activation:	gelu
mlp_ratio:	[1, 1]
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 256, 8, 8]          12,544
        PatchEmbed-2              [-1, 64, 256]               0
         LayerNorm-3              [-1, 64, 256]             512
            Linear-4             [-1, 256, 256]          16,640
              GELU-5             [-1, 256, 256]               0
           Dropout-6             [-1, 256, 256]               0
            Linear-7              [-1, 256, 64]          16,448
           Dropout-8              [-1, 256, 64]               0
               Mlp-9              [-1, 256, 64]               0
        LayerNorm-10              [-1, 64, 256]             512
           Linear-11              [-1, 64, 256]          65,792
             GELU-12              [-1, 64, 256]               0
          Dropout-13              [-1, 64, 256]               0
           Linear-14              [-1, 64, 256]          65,792
          Dropout-15              [-1, 64, 256]               0
              Mlp-16              [-1, 64, 256]               0
       MixerBlock-17              [-1, 64, 256]               0
        LayerNorm-18              [-1, 64, 256]             512
           Linear-19             [-1, 256, 256]          16,640
             GELU-20             [-1, 256, 256]               0
          Dropout-21             [-1, 256, 256]               0
           Linear-22              [-1, 256, 64]          16,448
          Dropout-23              [-1, 256, 64]               0
              Mlp-24              [-1, 256, 64]               0
        LayerNorm-25              [-1, 64, 256]             512
           Linear-26              [-1, 64, 256]          65,792
             GELU-27              [-1, 64, 256]               0
          Dropout-28              [-1, 64, 256]               0
           Linear-29              [-1, 64, 256]          65,792
          Dropout-30              [-1, 64, 256]               0
              Mlp-31              [-1, 64, 256]               0
       MixerBlock-32              [-1, 64, 256]               0
        LayerNorm-33              [-1, 64, 256]             512
           Linear-34             [-1, 256, 256]          16,640
             GELU-35             [-1, 256, 256]               0
          Dropout-36             [-1, 256, 256]               0
           Linear-37              [-1, 256, 64]          16,448
          Dropout-38              [-1, 256, 64]               0
              Mlp-39              [-1, 256, 64]               0
        LayerNorm-40              [-1, 64, 256]             512
           Linear-41              [-1, 64, 256]          65,792
             GELU-42              [-1, 64, 256]               0
          Dropout-43              [-1, 64, 256]               0
           Linear-44              [-1, 64, 256]          65,792
          Dropout-45              [-1, 64, 256]               0
              Mlp-46              [-1, 64, 256]               0
       MixerBlock-47              [-1, 64, 256]               0
        LayerNorm-48              [-1, 64, 256]             512
           Linear-49             [-1, 256, 256]          16,640
             GELU-50             [-1, 256, 256]               0
          Dropout-51             [-1, 256, 256]               0
           Linear-52              [-1, 256, 64]          16,448
          Dropout-53              [-1, 256, 64]               0
              Mlp-54              [-1, 256, 64]               0
        LayerNorm-55              [-1, 64, 256]             512
           Linear-56              [-1, 64, 256]          65,792
             GELU-57              [-1, 64, 256]               0
          Dropout-58              [-1, 64, 256]               0
           Linear-59              [-1, 64, 256]          65,792
          Dropout-60              [-1, 64, 256]               0
              Mlp-61              [-1, 64, 256]               0
       MixerBlock-62              [-1, 64, 256]               0
        LayerNorm-63              [-1, 64, 256]             512
           Linear-64                   [-1, 10]           2,570
================================================================
Total params: 678,410
Trainable params: 678,410
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 12.38
Params size (MB): 2.59
Estimated Total Size (MB): 14.97
----------------------------------------------------------------



